% Sablon pentru realizarea lucrarii de licenta, conform cu recomandarile
% din ghidul de redactare:
% - https://fmi.unibuc.ro/finalizare-studii/
% - https://drive.google.com/file/d/1xj9kZZgTkcKMJkMLRuoYRgLQ1O8CX0mv/view

% Multumiri lui Gabriel Majeri, acest sablon a fost creat pe baza
% codului sursa a lucrarii sale de licenta. 
% Codul sursa: https://github.com/GabrielMajeri/bachelors-thesis
% Website: https://www.gabrielmajeri.ro/
%
% Aceast sablon este licentiat sub Creative Commons Attribution 4.0 International License.

\documentclass[12pt, a4paper]{report}

% Suport pentru diacritice și alte  simboluri
\usepackage{fontspec}
\usepackage{soul}
% Suport pentru mai multe limbi
\usepackage{polyglossia}

% Setează limba textului la română
\setdefaultlanguage{romanian}
% Am nevoie de engleză pentru rezumat
\setotherlanguages{english}

% Indentează și primul paragraf al fiecărei noi secțiuni
\SetLanguageKeys{romanian}{indentfirst=true}

% Suport pentru diferite stiluri de ghilimele
\usepackage{csquotes}

\DeclareQuoteStyle{romanian}
  {\quotedblbase}
  {\textquotedblright}
  {\guillemotleft}
  {\guillemotright}

% Utilizează biblatex pentru referințe bibliografice
\usepackage[
    style=numeric,
    maxbibnames=50,
    sorting=nty
]{biblatex}

\addbibresource{bibliography.bib}
% Setează spațiere inter-linie la 1.5
\usepackage{setspace}
\onehalfspacing

% Modificarea geometriei paginii
\usepackage{geometry}

% Include funcțiile de grafică
\usepackage{graphicx}
% Încarcă imaginile din directorul `images`
\graphicspath{{./images/}}

% Listări de cod
\usepackage{listings}

% Linkuri interactive în PDF
\usepackage[
    colorlinks,
    linkcolor={black},
    menucolor={black},
    citecolor={black},
    urlcolor={blue}
]{hyperref}

% Simboluri matematice codificate Unicode
\usepackage[warnings-off={mathtools-colon,mathtools-overbracket}]{unicode-math}

% Comenzi matematice
\usepackage{amsmath}
\usepackage{mathtools}

% Formule matematice
\newcommand{\bigO}[1]{\symcal{O}\left(#1\right)}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

% Suport pentru rezumat în două limbi
% Bazat pe https://tex.stackexchange.com/a/70818
\newenvironment{abstractpage}
  {\cleardoublepage\vspace*{\fill}\thispagestyle{empty}}
  {\vfill\cleardoublepage}
\renewenvironment{abstract}[1]
  {\bigskip\selectlanguage{#1}%
   \begin{center}\bfseries\abstractname\end{center}}
  {\par\bigskip}

% Suport pentru anexe
\usepackage{appendix}

% Stiluri diferite de headere și footere
\usepackage{fancyhdr}

% Metadate
\title{Corectarea automată cu ajutorul LLM-urilor : Studiu de caz - Bacalaureat Informatică}
\author{Firca Liviu Nicolae}

% Generează variabilele cu @
\makeatletter

\begin{document}

% Front matter
\cleardoublepage
\let\ps@plain

% Pagina de titlu
\include{0-title}
\restoregeometry
\newgeometry{
    margin=2.5cm
}

\fancypagestyle{main}{
  \fancyhf{}
  \renewcommand\headrulewidth{0pt}
  \fancyhead[C]{}
  \fancyfoot[C]{\thepage}
}

\addtocounter{page}{1}

% Rezumatul
\include{0-abstract}

\tableofcontents

% Main matter
\cleardoublepage
\pagestyle{main}
\let\ps@plain\ps@main

\include{1-introducere}
\include{2-preliminarii}
\chapter{Colectarea și prelucrarea datelor}
\section{Colectarea datelor}
Datele au fost generate de studenții facultății. Modul în care au lucrat a fost următorul: li s-a asignat un subiect de bacalaureat la informatică din 2020 până în 2024 și au trebuit să își aleagă un număr aleatoriu dintre 5 și 10. După aceea au fost instruiți să facă greșeli astfel încât nota lor să corespundă cu numărul ales.  
Pentru a facilita corectarea lucrărilor, studenții au și trebuit să indice greșelile făcute, dar și nota pe care și-au asignat-o.  
Am primit în total 42 de lucrări, dintre care au fost filtrate 9 lucrări pentru că aveau date de proastă calitate.  
Problemele pe care le-am întâlnit aici erau multiple:  
\begin{itemize}
  \item Niște studenți pur și simplu nu au ales subiectul care trebuie (sesiunea greșită, profilul Științele Naturii în loc de Matematică-Informatică)
  \item Niște studenți au trimis în formatul greșit (fișier .docx a interferat cu sintaxa colului, fișiere Excel în loc de .txt)
  \item Niște studenți pur și simplu nu erau programatori puternici și nu s-au verificat destul, deci când a fost corectată lucrarea au
  avut mai multe greșeli decât au raportat. Acest lucru este problematic pentru că încetinește mult corectarea lucrărilor. Totuși, studenții erau din anul întâi și aceste greșeli au fost cel mai probabil neintenționate.
\end{itemize}

Pentru a evita aceste probleme puteam face 2 lucruri:
\begin{itemize}
  \item Să creăm un formular pentru submitere
  \item Să mobilizăm studenți cu mai multă experiență în programare
\end{itemize}


\section{Prelucrarea datelor}
Datele folosite au fost nu numai lucrările studenților, dar și subiectele și baremele fiecărui an. Acestea au trebuit să fie rescrise pentru a avea o structură mai adaptată pentru LLM-uri.  
În plus față de acestea, au fost rescrise într-un limbaj mai simplu și ușor de înțeles. Am filtrat a doua oară lucrările studenților până am avut 23 de lucrări în total. Acest lucru a fost făcut din două motive: a limita costurile lucrării (fiecare query costă relativ mult) și a permite testarea a mai multor LLM-uri, dar și pentru a avea date de calitate.  
Lucrările au fost recorectate, cu atenție sporită la barem, dar și la potențialele erori făcute de elev (tot conținutul lucrărilor a trebuit recorectat, pentru a garanta o notă cât mai apropiată de realitate).

\chapter{Corectarea automată și feedback automat}
\section{Corectarea automată}

Una dintre premisele lucrării este că, dacă LLM-ul are o eroare mai mică de 0.5 puncte din 10, atunci ar putea teoretic înlocui un corector uman.  
Ne întrebăm în primul rând dacă cea mai directă abordare (se dă toată lucrarea, tot baremul și tot subiectul și este instruit să noteze lucrarea pe baza baremului) produce rezultate bune.  
Pentru acest lucru o să folosim GPT-4.1 din două motive: nu este un thinking model (vrem să avem abordarea cât mai simplă) și este un model recent pentru studiu.  
Eroarea folosită este mean absolute error (MAE), dar se ia în calcul și cel mai prost rezultat — CMPR — când distanța între nota reală și cea asignată a fost cea mai mare.

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Modelul} & \textbf{MAE} & \textbf{CMPR} \\
\hline
GPT 4.1 & 4.78 & 18\\
\hline
\end{tabular}
\caption{Prima abordare, punctaj acordat din 100 de puncte}

\end{table}
Conform datelor, modelul se încadrează tehnic în limita superioară impusă (5 puncte din 100). Totuși, este foarte aproape de această limită, și CMPR este foarte mare.

În plus, analizând justificarea acordată a notelor, puteam observa mai multe lucruri în neregulă:
\begin{itemize}
  \item Uneori nu corectează pe baza baremului. De exemplu, dacă vede un program scris „foarte leneș”, îi va da 0 puncte, chiar dacă baremul generos de la bac i-ar acorda 5 puncte.
  \item Uneori, LLM-ul a rescris codul cu intenția de a-l puncta (lucru care se întâmplă dacă textul dat este foarte lung), dar a omis o parte a codului pe care 
  a judecat-o irelevantă. După ce a trecut prin cerințele baremului, a depunctat studentul pentru că lipsește partea de cod pe care nu a vrut să o copieze.
  Cred că lucrul acesta are două cauze simultane: un LLM nu se poate „gândi” la tot textul în același timp, deci nu își dă seama de eroarea comisă. În plus, planificarea în avans a fost clar proastă.
  \item Uneori nu și-a dat seama că răspunsul corect se afla în barem, deci a încercat să găsească singur soluția, dar nu a fost corectă. Deci corectarea a fost și ea greșită.
\end{itemize}


Multe dintre neînțelegerile sale au fost din cauza contextului prea mare, care l-a făcut să ignore baremul sau să corecteze greșit.  
Din cauza asta, avem o a doua abordare: ne dăm seama că fiecare întrebare de la bac este independentă, deci se împarte fiecare lucrare în 7 „lucrări”, iar baremul și subiectul
sunt și ele împărțite cum trebuie. După ce LLM-ul corectează tot, sunt 
recombinate toate cele 7 diviziuni în aceeași lucrare, și numai pe baza lucrării întregi se măsoară performanța. În plus folosim mai multe modele, inclusiv și thinking models. Din curiozitate, am inclus și GPT-4, pentru a vedea cât de important este factorul că modelele sunt noi.  
Folosind aceleași metrici:


\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Modelul} & \textbf{MAE} & \textbf{CMPR} \\
\hline
GPT 4.1 & 3.63 & 12\\

\hline
GPT 4 & 11.06 & 25.5 \\
\hline
Gemini 2.5 flash & 3.67 & 11\\


\hline
Gemini 2.5 pro & 3.09 & 11\\
\hline
Deepseek v3  & 5.8 & 14 \\

\hline
Deepseek r1 & 4.78 & 14 
\\
\hline

\end{tabular}
\caption{A doua abordare, punctaj acordat din 100 de puncte}

\end{table}

Putem remarca imediat că GPT-4 este un model foarte prost în comparație cu ceilalți, deci confirmă ideea că LLM-urile s-au ameliorat foarte mult în anii recenți  
și nu mai este cazul să folosim unul care nu este recent. Un LLM de actualitate este mult mai performant și, de multe ori, mult mai ieftin de folosit.  
Un alt lucru care iese la iveală este că resursele alocate inferenței sunt cruciale: există o diferență semnificativă între Gemini 2.5 Flash și Gemini 2.5 Pro de 0.5 puncte.  
În plus față de asta, există o diferență semnificativă între DeepSeek v3 și DeepSeek R1 de 1 punct, susținând iarăși ideea că mai multe resurse alocate la inferență rezultă în predicții mai bune.  
Deci se pot distinge doi factori cruciali în a determina performanța predicțiilor:
\begin{itemize}
  \item Resursele alocate inferenței
  \item Cât de recent este LLM-ul folosit (cu cât este mai recent, cu atât mai bine)
\end{itemize}


Putem remarca și faptul că GPT-4.1 și Gemini 2.5 Pro au avut erori foarte mici: 3.6 și 3. Acest lucru este destul de impresionant, pentru că există o parte subiectivă  
în a interpreta baremul, iar rezultatul cel mai bun (3) este cu mult sub limita superioară impusă. CMPR este totuși destul de mare pentru ambele modele.  
Două greșeli pe care am putut să le identific sunt următoarele:
\begin{itemize}
  \item Un student a făcut o greșeală în logica codului, iar Gemini 2.5 Pro a spus că algoritmul este greșit, când era clar că era doar o mică greșeală de logică.
  \item Un student nu a respectat cerința, dar GPT-4.1 i-a acordat toate punctele, pentru că nu și-a dat seama că structurile \textbf{repetă ... până când ...} și \textbf{până când ... repetă ...}  
  sunt foarte diferite în contextul cerinței: trebuia ca studentul să înlocuiască a doua structură în pseudocod cu prima, pentru a arăta că a înțeles cum se pot echivala algoritmic ambele.  
  Studentul nu a folosit structura care trebuie, dar a primit totuși toate punctele.
\end{itemize}

Ca soluție la aceste probleme, ar putea fi folosit un barem mai explicit, mai redundant, sau poate chiar și fine-tuning.



\section{Feedback automat}
Vrem să ne interesăm dacă LLM-urile pot să analizeze cum trebuie greșelile studenților
și să propună feedback relevant profesorului (ce parte din materie nu a fost bine înțeleasă de către studenți și trebuie predată mai bine).
Pentru asta, luăm cele top 2 modele în performanță și le testăm. Datele primite ca input pentru LLM sunt corectările lucrărilor obținute, fără subiect sau lucrarea propriu-zisă (a trebuit să justifice nota LLM-ul, deci există destul context doar în corectare).
Această parte este mult mai subiectivă decât cea precedentă, pentru că este greu să cuantifici calitatea unui feedback.

Problemele comune ale lucrărilor, identificate de mine, sunt:
\begin{itemize}
\item sintaxa lui C++
\item Algoritmica nu este înțeleasă bine
\item Întrebările nu sunt citite bine și, de multe ori, studenții s-au aruncat în a scrie o soluție fără să se gândească
\item I/O este implementat prost
\item Prelucrarea stringurilor (char[ ]) nu este bine înțeleasă
\item Studenții pierd foarte multe puncte la întrebările cu răspunsuri multiple
\end{itemize}

Problemele comune, identificate de GPT 4.1 sunt:
\begin{enumerate}
\item Algoritmica nu este înțeleasă bine
\item Studenții nu lucrează bine cu matrice (acest feedback este categoric fals)
\item Prelucrarea stringurilor (char[ ]) nu este bine înțeleasă
\item I/O este implementat prost
\item Parametrii referință (int f(\&n)) nu sunt bine folosiți
\item Inițializările sunt prost făcute
\end{enumerate}

Deci LLM-ul a identificat niște probleme foarte importante, unele care nu au fost identificate de mine. Totuși (2) este un feedback greșit.
Ceea ce lipsește în acest feedback este că sintaxa nu a fost bine înțeleasă și că studenții pierd prea multe puncte la întrebările cu răspuns multiplu.

Când vine vorba de Gemini 2.5 Pro, a identificat toate problemele pe care le-am văzut (cât si GPT 4.1), mai puțin faptul că studenții au pierdut prea multe puncte la întrebările cu răspuns multiplu.
Nu am primit feedback greșit din partea lui Gemini 2.5 Pro.

Pentru a completa partea de feedback,
le-am cerut LLM-urilor să genereze 30 de probleme,
pentru ca studenții să se poată antrena cu ele.
Din cele 30 generate de GPT 4.1, 10 au fost irelevante, și dintre cele 30 generate de Gemini, 7 au fost irelevante.

Putem trage concluzia că:
\begin{itemize}
\item Se pare că o performanță mai bună în MAE și CMPR la corectare
se transpune într-o performanță mai bună când vine vorba de feedback
\item Gemini 2.5 Pro este cel mai bun model, și la corectare automată, dar și la feedback
\end{itemize}

Totuși, ambele LLM-uri au dat feedback relevant.
\include{3-concluzii}

\printbibliography[heading=bibintoc]

\end{document}