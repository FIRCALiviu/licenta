% Sablon pentru realizarea lucrarii de licenta, conform cu recomandarile
% din ghidul de redactare:
% - https://fmi.unibuc.ro/finalizare-studii/
% - https://drive.google.com/file/d/1xj9kZZgTkcKMJkMLRuoYRgLQ1O8CX0mv/view

% Multumiri lui Gabriel Majeri, acest sablon a fost creat pe baza
% codului sursa a lucrarii sale de licenta. 
% Codul sursa: https://github.com/GabrielMajeri/bachelors-thesis
% Website: https://www.gabrielmajeri.ro/
%
% Aceast sablon este licentiat sub Creative Commons Attribution 4.0 International License.

\documentclass[12pt, a4paper]{report}

% Suport pentru diacritice și alte simboluri
\usepackage{fontspec}
\usepackage{soul}
% Suport pentru mai multe limbi
\usepackage{polyglossia}

% Setează limba textului la română
\setdefaultlanguage{romanian}
% Am nevoie de engleză pentru rezumat
\setotherlanguages{english}

% Indentează și primul paragraf al fiecărei noi secțiuni
\SetLanguageKeys{romanian}{indentfirst=true}

% Suport pentru diferite stiluri de ghilimele
\usepackage{csquotes}

\DeclareQuoteStyle{romanian}
  {\quotedblbase}
  {\textquotedblright}
  {\guillemotleft}
  {\guillemotright}

% Utilizează biblatex pentru referințe bibliografice
\usepackage[
    maxbibnames=50,
    sorting=nty
]{biblatex}

\addbibresource{bibliography.bib}

% Setează spațiere inter-linie la 1.5
\usepackage{setspace}
\onehalfspacing

% Modificarea geometriei paginii
\usepackage{geometry}

% Include funcțiile de grafică
\usepackage{graphicx}
% Încarcă imaginile din directorul `images`
\graphicspath{{./images/}}

% Listări de cod
\usepackage{listings}

% Linkuri interactive în PDF
\usepackage[
    colorlinks,
    linkcolor={black},
    menucolor={black},
    citecolor={black},
    urlcolor={blue}
]{hyperref}

% Simboluri matematice codificate Unicode
\usepackage[warnings-off={mathtools-colon,mathtools-overbracket}]{unicode-math}

% Comenzi matematice
\usepackage{amsmath}
\usepackage{mathtools}

% Formule matematice
\newcommand{\bigO}[1]{\symcal{O}\left(#1\right)}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

% Suport pentru rezumat în două limbi
% Bazat pe https://tex.stackexchange.com/a/70818
\newenvironment{abstractpage}
  {\cleardoublepage\vspace*{\fill}\thispagestyle{empty}}
  {\vfill\cleardoublepage}
\renewenvironment{abstract}[1]
  {\bigskip\selectlanguage{#1}%
   \begin{center}\bfseries\abstractname\end{center}}
  {\par\bigskip}

% Suport pentru anexe
\usepackage{appendix}

% Stiluri diferite de headere și footere
\usepackage{fancyhdr}

% Metadate
\title{Corectarea automata cu ajutorul LLM-urilor : Studiu de caz - Bacalaureat Informatică}
\author{Firca Liviu Nicolae}

% Generează variabilele cu @
\makeatletter

\begin{document}

% Front matter
\cleardoublepage
\let\ps@plain

% Pagina de titlu
\include{0-title}
\restoregeometry
\newgeometry{
    margin=2.5cm
}

\fancypagestyle{main}{
  \fancyhf{}
  \renewcommand\headrulewidth{0pt}
  \fancyhead[C]{}
  \fancyfoot[C]{\thepage}
}

\addtocounter{page}{1}

% Rezumatul
\include{0-abstract}

\tableofcontents

% Main matter
\cleardoublepage
\pagestyle{main}
\let\ps@plain\ps@main

\include{1-introducere}
\include{2-preliminarii}
\chapter{Colectarea si prelucrarea datelor}
\section{Colectarea datelor}
Datele au fost generate de studenti in aplicatia teams a facultatii. Modul in care au lucrat a fost urmatorul: li s-a asignat un subiect de bac la informatica din 2020 pana in 2024
si au trebuit sa isi aleaga un numar aleatoriu dintre 5 si 10. Dupa acea au fost instruiti sa faca greseli astfel incat nota lor sa corespunda cu numarul ales.
Pentru a facilita corectarea lucrarilor, studentii au si trebuit sa indice greselile facute, dar si nota pe care si-au asignat-o.
Am primit in total 42 de lucrari, dintre care au fost filtrate 9 lucrari pentru ca aveau date de proasta calitate.
Problemele pe care le-am intalnit aici erau multiple
\begin{itemize}
  \item Niste studenti pur si simplu nu au ales subiectul care trebuie (sesiunea gresita, profilul stiintele naturii in loc de Matematica-Informatica)
  \item Niste studenti au trimis in formatul gresit (fisier .docx a interferat cu sitaxa colului, fisiere excel in loc de .txt)
  \item Niste studenti pur si simplu nu erau programatori puternici, si nu s-au verificat destul , deci cand a fost efectiv corectata lucrarea au
  avut mai multe greseli decat au raportat. Acest lucru este problematic pentru ca incetineste mult corectarea lucrarilor. Totusi studentii erau din anul intai si aceste greseli au fost cel mai probabil neintentionate


\end{itemize} 

Pentru a evita aceste probleme puteam face 2 lucrui:
\begin{itemize}
  \item Sa creem un formular pentru submitere 
  \item Sa mobilizam studenti cu mai multa experienta in programare
\end{itemize}

\section{Prelucrarea datelor}
Datele folosite au fost nu numai lucrarile studentilor, dar si subiectele si barelmele fiecarui an. Acestea au trebuit sa fie rescrise pentru a avea o structura mai adapatata pentru LLM-uri.
In plus de acestea, au fost rescrise intr-un limbaj mai simplu si usor de inteles. Am filtrat o a doua oara lucrarile studentilor pana a avea 23 de lucrari in total. Acest lucru a fost facut din 2 motive : a limita costurile lucrarii (fiecare query costa relativ mult) si a permite 
testarea a mai multor LLM-uri, dar si pentru a avea date de calitate. 
Lucrarile au fost corectate de autor, cu atentie sporita la barem dar si la potentialele erori facute de elev (tot continutul lucrarilor a trebuit recorectat, pentru a garanta o nota cat mai apropiata de realitate)

\chapter{Corectarea automata si feedback automat}
\section{Corectarea automata}

Una dintre premisele lucrarii este ca daca LLM-ul are o distanta mai mica de 0.5 puncte din 10, atunci ar putea teoretic inlocui un corector uman.
Ne intrebam in primul rand daca cea mai directa aboradare (se da toata lucrarea,
tot baremul, si tot subiectul si este instruit sa noteze lucrarea pe baza baremului) produce rezultate bune.
Pentru acest lucru o sa folosim GPT 4.1 din 2 motive: Nu este un thinking model (vrem sa avem abordarea cat mai simpla), si este un model recent pentru studiu.
Eroarea folosita este mean absolute error (MAE), dar se ia in calcul si cel mai prost rezultat CMPR , cand distanta intre nota reala si cea asignata a fost cea mai mare

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Modelul} & \textbf{MAE} & \textbf{CMPR} \\
\hline
GPT 4.1 & 4.78 & 18\\
\hline
\end{tabular}
\caption{Prima aboradare, punctaj acordat din 100 puncte}

\end{table}

Cum putea vedea modelul se incadreaza tehnic in limita superioara impusa (5 puncte din 100). Totusi, este foarte aproape de aceasta limita, si CMPR este foarte mare.

In plus analizand justificarea acordata a notelor puteam observa mai multe lucruri in neregula:
\begin{itemize}
  \item Uneori nu corecteaza pe baza baremului, de exemplu daca vede un program scris "foarte lenes", el ii va da 0 puncte chiar daca baremul generos de la bac i-ar acorda 5 puncte 
  \item Uneori LLM-ul a rescris codul cu intentia de al puncta (lucru care se intampla daca textul dat este foarte lung), dar a omis o parte a coduli care 
  a judecat-o irelevanta. Dar dupa ce a trecut prin cerintele baremului, a depunctat studentul pentru ca lipseste partea a codului care nu a vrut sa o copieze.
  Cred ca lucrul acesta are 2 cauze simultane : un LLM nu se poate "gandii" la tot textul in acelasi timp, deci nu isi da seama de eroarea comisa. In plus, planificarea in avans a fost clar proasta.
  \item Uneori nu si-a dat seama ca raspunsul corect se afla in barem, deci a incercat sa gaseasca singur solutia, dar nu a fost correct. Deci corectarea a fost si ea gresita
\end{itemize}

Multe dintre neintelegerile sale au fost din cauza contextului prea mare, care l-a facut sa ignore baremul sau sa corecteze gresit. 
Din cauza asta avem o a doua abordare: ne dam seama ca fiecare intrebare de la bac este independenta, deci se imparte fiecare lucrare in 7 "lucrari", si baremul si subiectul
sunt si ele impartite cum trebuie. In plus de asta folosim mai multe modele, inclusiv si thinking models. Din curiozitate, am inclus si GPT 4, pentru a vedea cat de important este factorul ca modelelel sunt noi.
Folosind aceleasi metrici:

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Modelul} & \textbf{MAE} & \textbf{CMPR} \\
\hline
GPT 4.1 & 3.63 & 12\\

\hline
GPT 4 & 11.06 & 25.5 \\
\hline
Gemini 2.5 flash & 3.67 & 11\\


\hline
Gemini 2.5 pro & 3.09 & 11\\
\hline
Deepseek v3  & 5.8 & 14 \\

\hline
Deepseek r1 & 4.78 & 14 
\\
\hline

\end{tabular}
\caption{A doua abordare, punctaj acordat din 100 puncte}

\end{table}

Putem remarca imediat ca GPT 4 este un model foarte prost in comparare cu ceilalti, deci confirma idea ca LLM-urile s-au ameliorat foarte mult in anii recenti 
si nu mai este cazul de a folosi unul care nu este recent. Un LLM recent este mult mai performant si de multe ori mult mai ieftin de folosit.
Un alt lucru care iese la iveala este ca resursele alocate inferentei este crucial: exista o diferenta significativa intre Gemini 2.5 flash si Gemini 2.5 pro de 0.5 puncte.
In plus de asta exista o diferenta significativa intre Deepseek v3 si Deepseek r1 de 1 punct, iarasi sustinand idea ca mai multe resurse alocate la inferente rezulta la predictii mai bune.
Deci se pot distinge 4 factori cruciali in a determina performanta predictiilor:
\begin{itemize}
  \item Resursele alocate inferentei (putere de calcul \times timp)
  \item Cat de recent este LLM-ul folosit (cu atat este mai recent, cu atat mai bine)
\end{itemize}

Putem remarca si faptul ca GPT 4.1  si Gemini 2.5 pro au avut erori foarte mici: 3.6 si 3. Acest lucru este destul de impresionant pentru ca exista o parte subiectiva
in a interpreta baremul, si rezultatul cel mai bun (3) este cu mult sub limita superioara impusa. CMPR este totusi destul de mare pentru ambele modele.
Doua greseli pe care am putut sa le identific este ca :
\begin{itemize}
  \item O data un student a facut o greseala in logica codului, si Gemini 2.5 pro a spus ca algoritmul este gresit, cand era clar doar o mica greseala de logica
  \item O data un student nu a respectat cerinta, dar GPT 4.1 i-a acordat toate punctele, pentru ca nu si-a dat seama ca structurile \textbf{repeta ... pana cand ...} si \textbf{pana cand ... repeta ...}
  sunt foarte diferite in contextul cerintei : trebuia ca studentul sa inlocuiasca a doua structura in pseudocod cu prima, pentru a arata ca a inteles cum se pot echivala algoritmic ambele.
  Studentul nu a folosit structura care trebuie, dar a primit totusi toate punctele.
\end{itemize}

Ca solutie la aceste probleme, ar putea fi folosit un barem mai explicit, mai redundant, sau poate chiar si fine-tuning.


\section{Feedback automat}
Vrem sa ne interesam daca LLM-urile pot sa analizeze cum trebuie greselile studentilor,
si sa propuna feedback relevant profesorului (ce parte din materie nu a fost bine inteleasa de catre studenti, si trebuie predata mai bine).
Pentru asta, luam cele top 2 modele in performanta, si le testam.
Aceasta parte este mult mai subiectiva decat cea precedenta, pentru ca este greu sa cuantifici calitatea unui feedback.
Probelemele comune, identificate de mine sunt :
\begin{itemize}
  \item sintaxa lui C++
  \item Algoritmica nu este inteleasa bine
  \item Intrebarile nu sunt citite bine, si de multe ori studentii s-au aruncat in a scrie o solutie fara sa se gandeasca
  \item I/O este implementat prost 
  \item Prelucrarea stringurilor (char[ ]) nu este bine inteleasa
  \item Studentii pierd foarte mult puncte la intrebarile cu raspunsuri multiple


\end{itemize}

Problemele comune, identificate de GPT 4.1 sunt:
\begin{enumerate}
  \item Algoritmica nu este inteleasa bine
  \item  \st{Studentii au probleme cand lucreaza cu matrici} (acest lucru este categoric fals)
  \item Prelucrarea stringurilor (char[ ]) nu este bine inteleasa
  \item I/O este implementat prost
  \item Parametrii referinta (int f(\&n)) nu sunt bine folositi
  \item Initializarile sunt prost facute
\end{enumerate}

Deci LLM-ul a identificat niste probleme foarte importante, unele care nu au fost identificate de mine. Totusi (2) este un feedback gresit.
Cea ce lipseste in acest feedback este ca sintaxa nu a fost bine inteleasa, si ca studentii pierd prea multe puncte la intrebarile cu raspuns multiplu.

Cand vine vorba de gemini 2.5, a identificat toate problemele pe care le-am vazut, mai putin problema cu intrebarile cu raspuns multiplu.
Nu am primit feedback gresit din partea lui gemini 2.5 .

Pentru a completa partea de feedback,
le-am cerut LLM-urilor sa genereze 10 probleme,
pentru ca studentii sa se poata antrena cu ele. 
Din cele 10 generate de GPT 4.1, 3 au fost irelevante, si dintre cele 10 generate de gemini, numai 1 a fost irelevanta.

Putem trage concluzia ca :
\begin{itemize}
  \item Se pare ca o performanta mai buna in MAE si CMPR se transpune intr-o performanta mai buna cand vine vorba de feedback
  \item Gemini 2.5 este cel mai bune model, si la corectare automata, dar si la feedback
\end{itemize}

Totusi, ambele LLM-uri au dat feedback relevant.
\include{3-concluzii}

\printbibliography[heading=bibintoc]

\end{document}